{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "# Import scikit-learn modules for data preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import tensorflow modules for building and training the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, LSTM, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the path to the zipped folders\n",
    "# zip_file1 = \"/content/drive/MyDrive/Prototype/Dataset/Foul-20230406T015240Z-001.zip\"\n",
    "# zip_file2 = \"/content/drive/MyDrive/Prototype/Dataset/No Foul-20230406T015243Z-001.zip\"\n",
    "\n",
    "# # Extract the contents of the first zipped folder\n",
    "# with zipfile.ZipFile(zip_file1, \"r\") as zip_ref:\n",
    "#     zip_ref.extractall(\"/content/drive/MyDrive/Prototype/Dataset\")\n",
    "\n",
    "# # Extract the contents of the second zipped folder\n",
    "# with zipfile.ZipFile(zip_file2, \"r\") as zip_ref:\n",
    "#     zip_ref.extractall(\"/content/drive/MyDrive/Prototype/Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_skip = 5  # Set the number of frames to skip between extracted frames\n",
    "\n",
    "# Define the directory containing the video data\n",
    "data_dir = r\"/content/drive/MyDrive/Prototype/Dataset/\"\n",
    "\n",
    "# Define the classes\n",
    "classes = [\"Foul\", \"No Foul\"]\n",
    "\n",
    "data = []\n",
    "for idx, c in enumerate(classes):\n",
    "    path = os.path.join(data_dir, c)\n",
    "    print(\"Searching for videos in:\", path)\n",
    "    \n",
    "    videos = [file for file in os.listdir(path) if file.endswith(\".mp4\")]\n",
    "    print(\"Found {} videos for class {}\".format(len(videos), c))\n",
    "    \n",
    "    for v in videos:\n",
    "        video_path = os.path.join(path, v)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_count = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if frame_count % frame_skip == 0:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                frame = cv2.resize(frame, (224, 224))\n",
    "                data.append([frame, idx])\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "print(\"Number of frames read:\", len(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data list to numpy arrays for input and output data\n",
    "X = np.array([i[0] / 255.0 for i in data]) # Divide by 255.0 to normalize pixel values\n",
    "y = np.array([i[1] for i in data])\n",
    "\n",
    "# Print the length of the input and output data arrays\n",
    "print(\"Length of X:\", len(X))\n",
    "print(\"Length of y:\", len(y))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "print(\"Train dataset class distribution:\", dict(zip(unique_train, counts_train)))\n",
    "print(\"Test dataset class distribution:\", dict(zip(unique_test, counts_test)))\n",
    "\n",
    "\n",
    "# Reshape X_train and X_test to (samples, time_steps, width, height, channels)\n",
    "time_steps = 10  # Set the number of time steps to be used in the input\n",
    "X_train = np.reshape(X_train, (-1, time_steps, 224, 224, 1))\n",
    "X_test = np.reshape(X_test, (-1, time_steps, 224, 224, 1))\n",
    "\n",
    "# Convert string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "# Convert integer labels to one-hot encoded labels\n",
    "y_train_one_hot = to_categorical(y_train_encoded)\n",
    "y_test_one_hot = to_categorical(y_test_encoded)\n",
    "\n",
    "# Reshape y_train and y_test to match the new shapes of X_train and X_test\n",
    "y_train = np.reshape(y_train_one_hot, (-1, time_steps, len(classes)))\n",
    "y_test = np.reshape(y_test_one_hot, (-1, time_steps, len(classes)))\n",
    "\n",
    "# Take the first label for each sequence of frames\n",
    "y_train = y_train[:, 0, :]\n",
    "y_test = y_test[:, 0, :]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the layers for the model\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(time_steps, 224, 224, 1)))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(128, (3, 3), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(256, (3, 3), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "# Print the summary of the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the optimizer 'adam', using 'categorical_crossentropy' as the loss function and 'accuracy' as the evaluation metric.\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define EarlyStopping to stop training the model when the validation loss doesn't improve after 10 epochs.\n",
    "# Also, define ModelCheckpoint to save the best model based on validation accuracy.\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# Define ImageDataGenerator to perform data augmentation on the training data by rescaling, applying random shear and zoom, and performing horizontal flips.\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "\n",
    "\n",
    "# Define ImageDataGenerator to rescale the test data.\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Calculate the number of steps for training and testing data\n",
    "batch_size = 32\n",
    "train_steps = len(X_train) // batch_size\n",
    "test_steps = len(X_test) // batch_size\n",
    "\n",
    "def augment_data(X, y, datagen, batch_size):\n",
    "    X_reshaped = np.reshape(X, (-1, X.shape[2], X.shape[3], X.shape[4]))\n",
    "    y_repeated = np.repeat(y, X.shape[1], axis=0)\n",
    "    gen = datagen.flow(X_reshaped, y_repeated, batch_size=batch_size * X.shape[1])\n",
    "    while True:\n",
    "        X_augmented_batch, y_batch = next(gen)\n",
    "        yield X_augmented_batch.reshape((-1, X.shape[1], X.shape[2], X.shape[3], X.shape[4])), y_batch[:X_augmented_batch.shape[0] // X.shape[1]]\n",
    "\n",
    "train_generator = augment_data(X_train, y_train, train_datagen, batch_size=32)\n",
    "test_generator = augment_data(X_test, y_test, test_datagen, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, steps_per_epoch=train_steps, epochs=50, validation_data=test_generator, validation_steps=test_steps, callbacks=[es, mc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save('action_rec.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator, steps=test_steps, verbose=1)\n",
    "\n",
    "# Print the test set loss and accuracy\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get the predicted classes for the test dataset\n",
    "y_test_pred = model.predict(test_generator, steps=len(X_test), verbose=1)\n",
    "y_test_pred_classes = np.argmax(y_test_pred, axis=1)\n",
    "\n",
    "# Truncate the predicted classes array to have the same length as the true classes array\n",
    "y_test_pred_classes = y_test_pred_classes[:len(y_test_encoded)]\n",
    "\n",
    "# Inverse transform the integer-encoded labels back to their original string labels\n",
    "y_test_true_classes = np.array(classes)[y_test_encoded]\n",
    "y_test_pred_classes = np.array(classes)[y_test_pred_classes]\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test_true_classes, y_test_pred_classes, target_names=classes))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
